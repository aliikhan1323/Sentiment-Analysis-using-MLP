# ğŸ¬ IMDB Movie Review Sentiment Analysis using Deep Learning (Optimized ANN)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3%2B-green)
![NLP](https://img.shields.io/badge/NLP-Sentiment--Analysis-yellow)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

---

## ğŸ“˜ Project Overview

This project applies **Deep Learning** techniques to perform **Sentiment Analysis** on the **IMDB Movie Review Dataset** using an **Artificial Neural Network (ANN)**.  
The model determines whether a movie review expresses a **positive** or **negative** sentiment.

It uses **TF-IDF** for text feature extraction and a carefully optimized neural network with:
- **Batch Normalization**
- **Leaky ReLU Activation**
- **Dropout Regularization**
- **L2 Weight Regularization**
- **Early Stopping**

The architecture is fine-tuned to achieve **robust accuracy (~87%)** with controlled overfitting.

---

## ğŸ¯ Objective

To develop a **robust text classification model** that can accurately predict the sentiment polarity of IMDB movie reviews using optimized neural network architecture and advanced NLP preprocessing.

---

## ğŸ“‚ Dataset Information

**Dataset Name:** [IMDB Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)  
**Size:** 50,000 reviews (Balanced)  
- 25,000 for training  
- 25,000 for testing  

**Label Distribution:**
| Sentiment | Label | Count |
|------------|--------|--------|
| Positive | 1 | 25,000 |
| Negative | 0 | 25,000 |

Each review is a textual paragraph expressing a userâ€™s opinion about a movie.

---

## âš™ï¸ Project Workflow

### 1ï¸âƒ£ Data Preprocessing
- Convert all text to lowercase  
- Remove HTML tags and punctuation  
- Keep only alphabetic characters  
- Encode sentiment labels (positive â†’ 1, negative â†’ 0)

### 2ï¸âƒ£ Data Splitting
- 80% Training Data  
- 20% Testing Data  
- Random seed fixed at `42` for reproducibility

### 3ï¸âƒ£ Feature Extraction
- **TF-IDF Vectorization**
  - `max_features = 15000`
  - `ngram_range = (1, 2)`
  - `stop_words = 'english'`

This converts textual reviews into a **15,000-dimensional numeric vector**, representing word importance.

### 4ï¸âƒ£ Model Architecture

| Layer Type | Units | Activation | Regularization | Dropout | Notes |
|-------------|--------|-------------|----------------|----------|-------|
| Dense | 1024 | LeakyReLU(0.1) | L2(0.001) | 0.5 | Input Layer |
| Dense | 512 | LeakyReLU(0.1) | L2(0.001) | 0.4 | Hidden Layer |
| Dense | 256 | LeakyReLU(0.1) | L2(0.001) | 0.3 | Hidden Layer |
| Dense | 128 | LeakyReLU(0.1) | L2(0.001) | 0.3 | Hidden Layer |
| Dense | 64 | LeakyReLU(0.1) | L2(0.001) | 0.2 | Hidden Layer |
| Dense | 1 | Sigmoid | - | - | Output Layer |

Additional Enhancements:
- **BatchNormalization** after every layer for faster convergence  
- **LeakyReLU** prevents neuron death  
- **Dropout** ensures better generalization  
- **L2 Regularization** reduces weight explosion  

---

## ğŸ§  Model Compilation and Training

| Parameter | Value |
|------------|--------|
| Optimizer | Adam |
| Learning Rate | 0.0005 |
| Loss Function | Binary Crossentropy |
| Metric | Accuracy |
| Epochs | 15 (Early Stopping) |
| Batch Size | 64 |
| Validation Split | 20% |
| Early Stopping | Patience = 2 |

**Early Stopping:** Stops training when `val_loss` no longer improves, ensuring the best weights are restored.

---

## ğŸ“Š Results and Analysis

### âœ… Final Evaluation Metrics

| Metric | Score |
|--------|--------|
| **Accuracy** | 0.8712 |
| **Precision (Positive)** | 0.86 |
| **Recall (Positive)** | 0.88 |
| **F1-Score (Positive)** | 0.87 |
| **Precision (Negative)** | 0.88 |
| **Recall (Negative)** | 0.86 |
| **F1-Score (Negative)** | 0.87 |

---

### ğŸ” Classification Report

           precision    recall  f1-score   support

       0       0.88      0.86      0.87      4961
       1       0.86      0.88      0.87      5039

accuracy                           0.87     10000


### ğŸ§© Confusion Matrix

|              | Predicted Negative | Predicted Positive |
|--------------|--------------------|--------------------|
| **Actual Negative (0)** | 4255 | 706 |
| **Actual Positive (1)** | 582  | 4457 |

**Interpretation:**
- 4255 reviews were correctly identified as negative  
- 4457 reviews were correctly identified as positive  
- The model misclassified only around **12.9%** of the total reviews  

---

### ğŸ“ˆ Training Behavior

- **Training Accuracy:** â†‘ 93% â†’ 94%  
- **Validation Accuracy:** Stabilized around 87%  
- **Loss Curve:** Validation loss converges early due to L2 regularization and early stopping  
- **No overfitting** observed â€” training and validation performance remain closely aligned  

---

## ğŸ§° Technologies Used

### ğŸ”¹ Programming Language
- **Python 3.8+**

### ğŸ”¹ Data Handling & Preprocessing
- **pandas** â†’ Dataset manipulation  
- **numpy** â†’ Array operations  
- **re (Regex)** â†’ Text cleaning and pattern matching  

### ğŸ”¹ Natural Language Processing
- **scikit-learn**
  - `TfidfVectorizer` â†’ Text vectorization  
  - `train_test_split` â†’ Data partitioning  
  - `accuracy_score`, `classification_report`, `confusion_matrix` â†’ Performance metrics  

### ğŸ”¹ Deep Learning Framework
- **TensorFlow / Keras**
  - `Sequential`, `Dense`, `Dropout`, `BatchNormalization`, `LeakyReLU` â†’ Neural network architecture  
  - `Adam Optimizer` â†’ Adaptive gradient optimization  
  - `EarlyStopping` â†’ Regularization and convergence control  
  - `l2` â†’ Weight regularization  

### ğŸ”¹ Optional Visualization (Recommended)
- **Matplotlib** â†’ For plotting accuracy/loss curves  
- **Seaborn** â†’ For visualizing confusion matrix  

---
## ğŸš€ Future Improvements

| Enhancement | Description |
|--------------|-------------|
| ğŸ”¤ **Word Embeddings** | Replace TF-IDF with Word2Vec, GloVe, or FastText |
| ğŸ§© **Deep Architectures** | Use LSTM, GRU, or BiLSTM for sequential learning |
| ğŸŒ **Transfer Learning** | Integrate BERT or DistilBERT for contextual embeddings |
| ğŸ“ˆ **Visualization Dashboard** | Add training analytics via TensorBoard or Streamlit |
| ğŸ§® **Hyperparameter Optimization** | Tune learning rate, dropout, and regularization strength using Optuna |



---
ğŸ‘¨â€ğŸ’» Author

Ali Khan
AI Engineer 
ğŸ“§ alikhan132311@gmail.com


ğŸ’¡ Passionate about Deep Learning, NLP, and Model Optimization.

â­ If you find this project helpful, please give it a star on GitHub!
